{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d322b6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c7356b",
   "metadata": {},
   "source": [
    "# 1. CONFIGURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "becdbddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    GOLD_FILE = 'gold_interactions.csv'\n",
    "    \n",
    "    USER_BASED_FILE = 'user_based_results.csv'\n",
    "    ITEM_BASED_FILE = 'item_based_results.csv'\n",
    "    \n",
    "    # Evaluation constraints\n",
    "    K_PRECISION = [5, 10]\n",
    "    NDCG_K = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78e7ef4",
   "metadata": {},
   "source": [
    "# 2. METRICS IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d4faf9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_precision_at_k(recommended_list, gold_set, k):\n",
    "    top_k = recommended_list[:k]\n",
    "    if not top_k:\n",
    "        return 0.0\n",
    "    relevant_cnt = sum(1 for item in top_k if item in gold_set)\n",
    "    return relevant_cnt / k\n",
    "\n",
    "def calculate_ndcg(recommended_list, gold_ratings_dict, k):\n",
    "    dcg = 0.0\n",
    "    top_k = recommended_list[:k]\n",
    "    \n",
    "    for i, item in enumerate(top_k):\n",
    "        rel = gold_ratings_dict.get(item, 0.0)\n",
    "        gain = (2**rel - 1)\n",
    "        discount = math.log2((i + 1) + 1)\n",
    "        dcg += gain / discount\n",
    "        \n",
    "    # Calculate Ideal DCG\n",
    "    ideal_ratings = sorted(gold_ratings_dict.values(), reverse=True)[:k]\n",
    "    idcg = 0.0\n",
    "    for i, rel in enumerate(ideal_ratings):\n",
    "        gain = (2**rel - 1)\n",
    "        discount = math.log2((i + 1) + 1)\n",
    "        idcg += gain / discount\n",
    "        \n",
    "    if idcg == 0: return 0.0\n",
    "    return dcg / idcg\n",
    "\n",
    "def calculate_spearman(recommended_list, gold_ratings_dict):\n",
    "    xs = [] \n",
    "    ys = []\n",
    "    for rank_idx, item in enumerate(recommended_list):\n",
    "        if item in gold_ratings_dict:\n",
    "            xs.append(rank_idx + 1)\n",
    "            ys.append(gold_ratings_dict[item])\n",
    "            \n",
    "    n = len(xs)\n",
    "    if n < 2: return 0.0\n",
    "    \n",
    "    x = np.array(xs)\n",
    "    y = np.array(ys)\n",
    "    \n",
    "    x_mean = np.mean(x)\n",
    "    y_mean = np.mean(y)\n",
    "    \n",
    "    numerator = np.sum((x - x_mean) * (y - y_mean))\n",
    "    denom_x = np.sqrt(np.sum((x - x_mean)**2))\n",
    "    denom_y = np.sqrt(np.sum((y - y_mean)**2))\n",
    "    \n",
    "    if denom_x == 0 or denom_y == 0: return 0.0\n",
    "    return numerator / (denom_x * denom_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64803481",
   "metadata": {},
   "source": [
    "# 3. EVALUATION RUNNER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "257453a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gold_data():\n",
    "    print(\"Loading Gold Data...\")\n",
    "    df = pd.read_csv(Config.GOLD_FILE)\n",
    "    gold_data = {}\n",
    "    for _, row in df.iterrows():\n",
    "        uid = int(row['user_id'])\n",
    "        bid = int(row['book_id'])\n",
    "        rating = float(row['rating'])\n",
    "        \n",
    "        if uid not in gold_data: gold_data[uid] = {}\n",
    "        gold_data[uid][bid] = rating\n",
    "    return gold_data\n",
    "\n",
    "def evaluate_file(filename, gold_data, algorithm_name):\n",
    "    print(f\"\\n--- Evaluating {algorithm_name} ---\")\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(filename)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Could not find {filename}. Run main.py first.\")\n",
    "        return\n",
    "    \n",
    "    metrics = {'P@5': [], 'P@10': [], 'NDCG': [], 'Spearman': []}\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        uid = int(row['user_id'])\n",
    "        \n",
    "        if pd.isna(row['recommendations']):\n",
    "            recs = []\n",
    "        else:\n",
    "            # Handle potential float/string formatting issues in CSV\n",
    "            recs_str = str(row['recommendations']).strip()\n",
    "            if not recs_str:\n",
    "                recs = []\n",
    "            else:\n",
    "                recs = [int(float(x)) for x in recs_str.split()]\n",
    "            \n",
    "        user_gold_dict = gold_data.get(uid, {})\n",
    "        user_gold_set = set(user_gold_dict.keys())\n",
    "        \n",
    "        if not user_gold_dict: continue\n",
    "            \n",
    "        metrics['P@5'].append(calculate_precision_at_k(recs, user_gold_set, 5))\n",
    "        metrics['P@10'].append(calculate_precision_at_k(recs, user_gold_set, 10))\n",
    "        metrics['NDCG'].append(calculate_ndcg(recs, user_gold_dict, Config.NDCG_K))\n",
    "        metrics['Spearman'].append(calculate_spearman(recs, user_gold_dict))\n",
    "        \n",
    "    print(f\"Results for {algorithm_name}:\")\n",
    "    print(f\"Mean P@5:      {np.mean(metrics['P@5']):.4f}\")\n",
    "    print(f\"Mean P@10:     {np.mean(metrics['P@10']):.4f}\")\n",
    "    print(f\"Mean NDCG:     {np.mean(metrics['NDCG']):.4f}\")\n",
    "    print(f\"Mean Spearman: {np.mean(metrics['Spearman']):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe886e2",
   "metadata": {},
   "source": [
    "# 4. MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3527e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Gold Data...\n",
      "\n",
      "--- Evaluating User-Based CF ---\n",
      "Results for User-Based CF:\n",
      "Mean P@5:      0.0384\n",
      "Mean P@10:     0.0298\n",
      "Mean NDCG:     0.0384\n",
      "Mean Spearman: -0.0026\n",
      "\n",
      "--- Evaluating Item-Based CF ---\n",
      "Results for Item-Based CF:\n",
      "Mean P@5:      0.0056\n",
      "Mean P@10:     0.0060\n",
      "Mean NDCG:     0.0055\n",
      "Mean Spearman: -0.0040\n",
      "\n",
      "***Evaluation Complete.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    gold_data = load_gold_data()\n",
    "    \n",
    "    # Evaluate Method 1: User-Based\n",
    "    evaluate_file(Config.USER_BASED_FILE, gold_data, \"User-Based CF\")\n",
    "    \n",
    "    # Evaluate Method 2: Item-Based\n",
    "    evaluate_file(Config.ITEM_BASED_FILE, gold_data, \"Item-Based CF\")\n",
    "    \n",
    "    print(\"\\n***Evaluation Complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
